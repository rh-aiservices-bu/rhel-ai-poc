{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb24d2-f2f7-455f-8854-6d1ae1f233b8",
   "metadata": {},
   "source": [
    "# Evaluating the fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a3532-1ed4-4671-bea4-5e35f64da2bf",
   "metadata": {},
   "source": [
    "### Model inference parameters\n",
    "\n",
    "The parameters to the fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b4bcd-ed99-4d70-aa8a-b4586a9a4966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:42:29.904601Z",
     "start_time": "2024-12-18T19:42:29.895311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Iterator\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "\n",
    "def replace_special_char(original_str):\n",
    "    return re.sub(r\"[^\\w]\", \"_\", original_str)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    with open(\"llm_config.yaml\", \"r\") as f:\n",
    "        llm_config = yaml.safe_load(f)\n",
    "    return llm_config\n",
    "\n",
    "\n",
    "def get_output_dir(parent=\"\"):\n",
    "    llm_config = get_config()\n",
    "    output_directory = replace_special_char(llm_config.get(\"name\", \"output\"))\n",
    "    if parent:\n",
    "        output_directory = f\"{parent}/{output_directory}\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    return output_directory\n",
    "\n",
    "\n",
    "def get_testing_config_name(testing_config):\n",
    "    name = testing_config.get(\"name\")\n",
    "    if name:\n",
    "        return replace_special_char(name)\n",
    "\n",
    "    name = testing_config.get(\"model_name\")\n",
    "    if name and testing_config.get(\"rag\"):\n",
    "        name = replace_special_char(name + \"_rag\")\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fa004651738c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:42:32.092469Z",
     "start_time": "2024-12-18T19:42:32.085639Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca2a6-523d-4a83-a102-270f5747bb00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:42:32.765600Z",
     "start_time": "2024-12-18T19:42:32.763343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS = 2048\n",
    "TEMPERATURE = 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2c060-ea3e-4bbe-8f3a-3c2e5605c2e4",
   "metadata": {},
   "source": [
    "### Milvus connection info\n",
    "\n",
    "Defaults to local db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655969f-53fb-4917-8a7e-1ee480402524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:42:40.665693Z",
     "start_time": "2024-12-18T19:42:40.662983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_URI = os.getenv(\"MILVUS_URI\", \"./milvus_llm_judge_eval.db\")\n",
    "MILVUS_USERNAME = os.getenv(\"MILVUS_USERNAME\", \"\")\n",
    "MILVUS_PASSWORD = os.getenv(\"MILVUS_PASSWORD\", \"\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\", \"my_org_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420575b-4d00-458b-aa0e-7030008ccd53",
   "metadata": {},
   "source": [
    "## Sanity check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e7d20fe421c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:42:46.102321Z",
     "start_time": "2024-12-18T19:42:46.098752Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_llm(testing_config):\n",
    "    openai_api_key = re.sub(r\"\\s+\", \"\", testing_config.get(\"api_key\"))\n",
    "    model_name = testing_config.get(\"model_name\")\n",
    "    if testing_config.get(\"model_type\") == \"openai\":\n",
    "        # print(\"Creating OpenAI model\")\n",
    "        return ChatOpenAI(\n",
    "            openai_api_key=openai_api_key,\n",
    "            model_name=model_name,\n",
    "            streaming=False)\n",
    "    # print(\"Creating VLLM model\")\n",
    "    openai_api_base = testing_config.get(\"endpoint_url\")\n",
    "    return VLLMOpenAI(\n",
    "        openai_api_key=openai_api_key,\n",
    "        openai_api_base=openai_api_base,  #https://model...com/v1\n",
    "        model_name=model_name,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "\n",
    "def qna_request(llm, template_str, question):\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            prompt = PromptTemplate.from_template(template_str)\n",
    "            chain = prompt | llm | StrOutputParser()\n",
    "            answer = chain.invoke({\"question\": question})\n",
    "            # print(answer)\n",
    "            return answer.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt + 1 < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2ab24a267887b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:44:52.340380Z",
     "start_time": "2024-12-18T19:44:47.595771Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    if testing_config.get(\"rag\"):\n",
    "        continue\n",
    "    else:\n",
    "        print(testing_config.get(\"name\"))\n",
    "        llm = create_llm(testing_config)\n",
    "        template_str = testing_config.get(\"template\")\n",
    "        question = \"Who are you?\"\n",
    "        answer = qna_request(llm, template_str, question)\n",
    "        print(answer)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961a7ae-d9d9-4cbc-9926-e1acaa6bc0f0",
   "metadata": {},
   "source": [
    "## Creating an Milvus DB with documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f598eb-8665-4240-91c0-3cc178aad88c",
   "metadata": {},
   "source": [
    "## Initial index creation and document ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff5f7-c509-48db-90b5-e15815b8b530",
   "metadata": {},
   "source": [
    "#### Load pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db9a89a16c5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoclingPDFLoader(BaseLoader):\n",
    "    def __init__(self, file_path: str | list[str]) -> None:\n",
    "        self._file_paths = file_path if isinstance(file_path, list) else [file_path]\n",
    "        self._converter = DocumentConverter()\n",
    "\n",
    "    def lazy_load(self) -> Iterator[LCDocument]:\n",
    "        for source in self._file_paths:\n",
    "            dl_doc = self._converter.convert(source).document\n",
    "            text = dl_doc.export_to_markdown()\n",
    "            yield LCDocument(page_content=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a968a5dd3653176",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"../data_preparation/document_collection\"\n",
    "file_paths = [str(path) for path in Path(pdf_folder_path).rglob('*.pdf')]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a38809-2915-4376-a847-0cec2abbfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DoclingPDFLoader(file_path=file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fe0a-38bf-4cd4-af7d-35b41c645edd",
   "metadata": {},
   "source": [
    "#### Split documents into chunks with some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362934767fd34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-c670-4eb5-803b-b4d591fa83db",
   "metadata": {},
   "source": [
    "#### Create the index and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb6d94-8615-4d55-b5b1-102f9ce56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": device}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": MILVUS_URI,\n",
    "        \"user\": MILVUS_USERNAME,\n",
    "        \"password\": MILVUS_PASSWORD\n",
    "    },\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    auto_id=True,\n",
    "    drop_old=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf425b-dffd-4f42-9537-49d41383182d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded = db.add_documents(splits)\n",
    "print(f\"{len(loaded)} documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3b458-4979-46df-8493-7496764a2568",
   "metadata": {},
   "source": [
    "#### Test vector DB search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c6e6d-c42c-4de4-87cf-8edfd0e63da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Who are you?\"\n",
    "# query = \"Who are the funding partners for the State's transformative infrastructure projects?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90feeb37-7888-4c5f-a5cb-5f82637cec16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d47df1-ec9b-4571-8864-0c2ff1051dbf",
   "metadata": {},
   "source": [
    "#### Test out RAG request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b702a5f57bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def rag_request(llm, template_str, question):\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            prompt = PromptTemplate.from_template(template_str)\n",
    "            rag_chain = (\n",
    "                    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                    | prompt\n",
    "                    | llm\n",
    "                    | StrOutputParser()\n",
    "            )\n",
    "            response = rag_chain.invoke(question)\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d018583d742b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    if not testing_config.get(\"rag\"):\n",
    "        continue\n",
    "    else:\n",
    "        llm = create_llm(testing_config)\n",
    "        template_str = testing_config.get(\"template\")\n",
    "        question = \"Who are you?\"\n",
    "        answer = rag_request(llm, template_str, question)\n",
    "        print(answer)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe9c8d763f89d3",
   "metadata": {},
   "source": [
    "## Generate Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44deb49353ef15",
   "metadata": {},
   "source": [
    "### Use qna.yaml, csv, jsonl to create some questions and ground truth answers\n",
    "\n",
    "We create a pandas dataframe with the columns `question` and `ground_truth`\n",
    "- create a csv file in the correct (default is \"ground_truth\") directory with the columns `question` and `ground_truth`\n",
    "\n",
    "- qna.yaml files can be taken as written from data_preparation and converted to the appropriate format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d59e9b14117335",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "qround_truth_df = pd.DataFrame(columns=[\"user_input\", \"reference\"])\n",
    "\n",
    "directory = \"reference_answers\"\n",
    "for file_path in Path(directory).rglob('*.csv'):\n",
    "    # print(file_path)\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    print(f\"{file_path}: {csv_df.shape[0]} questions\")\n",
    "    qround_truth_df = pd.concat([qround_truth_df, csv_df], ignore_index=True)\n",
    "\n",
    "for file_path in Path(directory).rglob('*.jsonl'):\n",
    "    # print(file_path)\n",
    "    jsonl_df = pd.read_json(file_path, orient=\"records\", lines=True)\n",
    "    print(f\"{file_path}: {jsonl_df.shape[0]} questions\")\n",
    "    qround_truth_df = pd.concat([qround_truth_df, jsonl_df], ignore_index=True)\n",
    "\n",
    "qna_list = []\n",
    "\n",
    "for file_path in Path(directory).rglob('*.yaml'):\n",
    "    with open(file_path) as file:\n",
    "        qna = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        for seed_example in qna[\"seed_examples\"]:\n",
    "            for questions_and_answers in seed_example[\"questions_and_answers\"]:\n",
    "                qna_list.append({\n",
    "                    \"user_input\": questions_and_answers[\"question\"].strip(),\n",
    "                    \"reference\": questions_and_answers[\"answer\"].strip()\n",
    "                })\n",
    "        print(f\"{file_path}: {len(qna_list)} questions\")\n",
    "\n",
    "ground_truth_df = pd.concat([qround_truth_df, pd.DataFrame(qna_list)], ignore_index=True)\n",
    "ground_truth_df = ground_truth_df.drop_duplicates(subset=[\"user_input\"])\n",
    "print(f\"{ground_truth_df.shape[0]} total unique questions\")\n",
    "\n",
    "ground_truth_df.to_json(f\"{output_directory}/reference_answers.jsonl\", orient=\"records\", lines=True)\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621433ec840ce28b",
   "metadata": {},
   "source": [
    "## Get responses from each of the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba1106-731a-4477-877e-2c0769fb6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "output_directory = get_output_dir(\"\")\n",
    "reference_answers_df = pd.read_json(f\"{output_directory}/reference_answers.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    print(\"-\" * 80)\n",
    "    print(testing_config.get(\"name\") or testing_config.get(\"model_name\"))\n",
    "    responses = reference_answers_df.copy()\n",
    "    responses[\"response\"] = \"\"\n",
    "    responses[\"rag_response\"] = \"\"\n",
    "    llm = create_llm(testing_config)\n",
    "    for index, row in responses.iterrows():\n",
    "        question = row[\"user_input\"]\n",
    "        print(index, question[:40])\n",
    "        if testing_config.get(\"rag\"):\n",
    "            answer = rag_request(llm, testing_config.get(\"template\"), question)\n",
    "        else:\n",
    "            answer = qna_request(llm, testing_config.get(\"template\"), question)\n",
    "        print(\"Answer: \" + answer[:40])\n",
    "        responses.at[index, \"response\"] = answer\n",
    "    testing_config_name = get_testing_config_name(testing_config)\n",
    "    responses.to_json(f\"{output_directory}/{testing_config_name}_responses.jsonl\", orient=\"records\", lines=True)\n",
    "    # responses.to_csv(f\"{output_directory}/{base_filename}_answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf0670-0bea-4807-b760-bf442824a17a",
   "metadata": {},
   "source": [
    "## Grade responses using OpenAI ChatGPT-4o as a Judge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa5c97-b204-474d-a395-f3acbfb50483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:50:11.766398Z",
     "start_time": "2024-12-18T19:50:11.749168Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "responses_directory = get_output_dir()\n",
    "output_directory = responses_directory + \"/openai_scores\"\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ce1c33dbb90e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:50:12.599942Z",
     "start_time": "2024-12-18T19:50:12.597294Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "scoring_template_str = llm_config[\"judge\"].get(\"template\")\n",
    "assert scoring_template_str\n",
    "SCORING_PROMPT = PromptTemplate.from_template(scoring_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510b781729a4cfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T19:50:14.022581Z",
     "start_time": "2024-12-18T19:50:14.010110Z"
    }
   },
   "outputs": [],
   "source": [
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "\n",
    "\n",
    "def score_request(question, answer, reference_answer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SCORING_PROMPT.format(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "                reference_answer=reference_answer\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = judge_client.chat.completions.create(\n",
    "        model=judge_model_name,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    response_content = re.sub(r'^```json', '', response_content)\n",
    "    response_content = re.sub(r'```$', '', response_content)\n",
    "    # print(response_content)\n",
    "    try:\n",
    "        result = json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        result = {\"answer_quality\": 0, \"reasoning\": \"Error\"}\n",
    "        print(\"response_content:\", response_content)\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    score = result[\"answer_quality\"]\n",
    "    reasoning = result[\"reasoning\"]\n",
    "    return score, reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792476cc472fa9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T20:03:00.337068Z",
     "start_time": "2024-12-18T19:50:33.603186Z"
    }
   },
   "outputs": [],
   "source": [
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    testing_config_name = get_testing_config_name(testing_config)\n",
    "    print(\"-\" * 80)\n",
    "    print(testing_config_name)\n",
    "\n",
    "    responses_filename = f\"{responses_directory}/{testing_config_name}_responses.jsonl\"\n",
    "    scores = pd.read_json(responses_filename, orient=\"records\", lines=True)\n",
    "    scores[\"score\"] = None\n",
    "    scores[\"reasoning\"] = None\n",
    "\n",
    "    for index, row in scores.iterrows():\n",
    "        user_input = row[\"user_input\"]\n",
    "        response = row[\"response\"]\n",
    "        reference_answer = row[\"reference\"]\n",
    "        print(index, user_input)\n",
    "        if answer:\n",
    "            score, reasoning = score_request(user_input, response, reference_answer)\n",
    "            scores.at[index, \"score\"] = score\n",
    "            scores.at[index, \"reasoning\"] = reasoning\n",
    "            print(response[:40], score, reasoning[:40])\n",
    "\n",
    "    judge_name = replace_special_char(judge_model_name)\n",
    "    scores_filename = f\"{output_directory}/{testing_config_name}_openai_scores\"\n",
    "    scores.to_json(f\"{scores_filename}.jsonl\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9ad25-b957-4fab-aea9-b9b828e030d2",
   "metadata": {},
   "source": [
    "## Create resulting score report CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74899d774f908db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T20:04:34.104057Z",
     "start_time": "2024-12-18T20:04:34.091027Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "output_directory = get_output_dir() + \"/openai_scores\"\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cedae-4bbf-419d-87cb-d5aedf404694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T20:07:21.388404Z",
     "start_time": "2024-12-18T20:07:21.362678Z"
    }
   },
   "outputs": [],
   "source": [
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "judge_name = replace_special_char(judge_model_name)\n",
    "\n",
    "summary_output_df = pd.DataFrame()\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    testing_config_name = get_testing_config_name(testing_config)\n",
    "    scores_filename = f\"{output_directory}/{testing_config_name}_openai_scores.jsonl\"\n",
    "    print(f\"Adding {scores_filename}\")\n",
    "    scores = pd.read_json(scores_filename, orient=\"records\", lines=True)\n",
    "    summary_output_df[f\"{testing_config_name}_score\"] = scores[\"score\"]\n",
    "\n",
    "average_row = summary_output_df.mean(axis=0, numeric_only=True)\n",
    "print(average_row)\n",
    "summary_output_df.loc[len(summary_output_df)] = average_row\n",
    "question_indices = [f\"Q{i + 1}\" for i in range(len(summary_output_df) - 1)]\n",
    "question_indices.append(\"Average\")\n",
    "summary_output_df.insert(0, 'question index', question_indices)\n",
    "\n",
    "summary_filepath = f\"{output_directory}/openai_summary_scores\"\n",
    "# summary_output_df.to_json(f\"{summary_filepath}.jsonl\", orient=\"records\", lines=True)\n",
    "summary_output_df.to_csv(f\"{summary_filepath}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bb2e5bde823e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T20:07:22.330005Z",
     "start_time": "2024-12-18T20:07:22.282585Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f\"{output_directory}/openai_scores.xlsx\") as writer:\n",
    "    summary_output_df = pd.read_csv(f\"{summary_filepath}.csv\")\n",
    "    summary_output_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "    for testing_config in llm_config[\"testing_config\"]:\n",
    "        testing_config_name = get_testing_config_name(testing_config)\n",
    "        scores_filename = f\"{output_directory}/{testing_config_name}_openai_scores.jsonl\"\n",
    "        scores = pd.read_json(scores_filename, orient=\"records\", lines=True)\n",
    "        scores.to_excel(writer, sheet_name=f\"{testing_config_name}_openai_scores\"[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2192c3-477f-4fa3-ae04-4222e8a4d979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
