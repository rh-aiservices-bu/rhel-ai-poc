{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb24d2-f2f7-455f-8854-6d1ae1f233b8",
   "metadata": {},
   "source": [
    "# Evaluating the fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "!pip install einops~=0.8.0 \n",
    "!pip install langchain~=0.3.5 \n",
    "!pip install langchain-community~=0.3.3 \n",
    "!pip install langchain-openai~=0.2.4\n",
    "!pip install langchain-milvus~=0.1.6\n",
    "!pip install pypdf~=5.1.0\n",
    "!pip install pymilvus~=2.4.9\n",
    "!pip install sentence-transformers~=3.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a3532-1ed4-4671-bea4-5e35f64da2bf",
   "metadata": {},
   "source": [
    "### Model inference parameters\n",
    "\n",
    "The parameters to the fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca2a6-523d-4a83-a102-270f5747bb00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "INFERENCE_SERVER_URL = os.getenv(\"INFERENCE_SERVER_URL\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "MAX_TOKENS=2048\n",
    "TEMPERATURE=0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2c060-ea3e-4bbe-8f3a-3c2e5605c2e4",
   "metadata": {},
   "source": [
    "### Milvus connection info\n",
    "\n",
    "Defaults to local db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655969f-53fb-4917-8a7e-1ee480402524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_URI = os.getenv(\"MILVUS_URI\", \"./milvus_local.db\")\n",
    "MILVUS_USERNAME = os.getenv(\"MILVUS_USERNAME\", \"\")\n",
    "MILVUS_PASSWORD = os.getenv(\"MILVUS_PASSWORD\", \"\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\", \"my_org_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b4bcd-ed99-4d70-aa8a-b4586a9a4966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_milvus import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420575b-4d00-458b-aa0e-7030008ccd53",
   "metadata": {},
   "source": [
    "## Sanity check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69188d06-d31e-4239-8be8-fdda029bfc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=LLM_API_KEY,\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "template_str=\"\"\"<|system|>\n",
    "You are a Red Hat Instruct Model based on Granite 7B,\n",
    "an AI language model developed by Red Hat and IBM Research,\n",
    "based on the Granite-7b-base language model.\n",
    "Your primary function is to be a chat assistant.\n",
    "<|user|>\n",
    "Answer the following question.\n",
    "Question: {question}\n",
    "Answer:\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651d905-f5a8-401d-aa38-4eb60a2f1046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947ce77-5459-48c1-a73e-35f886e4167a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"Hello.  Who are you?\"\n",
    "question = \"Which hardware accelerators are supported by RHEL AI?\"\n",
    "\n",
    "answer = conversation.predict(question=question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961a7ae-d9d9-4cbc-9926-e1acaa6bc0f0",
   "metadata": {},
   "source": [
    "## Creating an Milvus DB with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600cd763-6ecc-4c77-89c0-47108c31c44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_milvus import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f598eb-8665-4240-91c0-3cc178aad88c",
   "metadata": {},
   "source": [
    "## Initial index creation and document ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff5f7-c509-48db-90b5-e15815b8b530",
   "metadata": {},
   "source": [
    "#### Load pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4074d4-eff4-45b2-902d-ec8c075a83ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_folder_path = \"../data_preparation/document_collection\"\n",
    "\n",
    "pdf_loader = PyPDFDirectoryLoader(pdf_folder_path, recursive=True)\n",
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fe0a-38bf-4cd4-af7d-35b41c645edd",
   "metadata": {},
   "source": [
    "#### Split documents into chunks with some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba4a08-2194-4df1-9091-6f2b596757a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = pdf_docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-c670-4eb5-803b-b4d591fa83db",
   "metadata": {},
   "source": [
    "#### Create the index and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb6d94-8615-4d55-b5b1-102f9ce56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": device}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": MILVUS_URI,\n",
    "        \"user\": MILVUS_USERNAME, \n",
    "        \"password\": MILVUS_PASSWORD\n",
    "    },\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    auto_id=True,\n",
    "    drop_old=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf425b-dffd-4f42-9537-49d41383182d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded = db.add_documents(all_splits)\n",
    "print(f\"{len(loaded)} documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3b458-4979-46df-8493-7496764a2568",
   "metadata": {},
   "source": [
    "#### Test vector DB search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c6e6d-c42c-4de4-87cf-8edfd0e63da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Which hardware accelerators are supported by RHEL AI?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90feeb37-7888-4c5f-a5cb-5f82637cec16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d47df1-ec9b-4571-8864-0c2ff1051dbf",
   "metadata": {},
   "source": [
    "#### Test out RAG request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885afce-ef98-4e60-bb06-1c8307c13e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_template=\"\"\"<|system|>\n",
    "You are a Red Hat Instruct Model based on Granite 7B,\n",
    "an AI language model developed by Red Hat and IBM Research,\n",
    "based on the Granite-7b-base language model.\n",
    "Your primary function is to be a chat assistant.\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "Answer the following question from context and internal memory.\n",
    "Question: {question}\n",
    "Answer:\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# RAG_CHAIN_PROMPT = PromptTemplate(input_variables=[\"input\"], template=rag_template)\n",
    "RAG_CHAIN_PROMPT = PromptTemplate.from_template(rag_template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "            ),\n",
    "        chain_type_kwargs={\"prompt\": RAG_CHAIN_PROMPT},\n",
    "        return_source_documents=True\n",
    "        )\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d4642-0095-48ea-acb4-6bc3c73433e4",
   "metadata": {},
   "source": [
    "#### RAG query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a7c3c-cd0a-4454-b2fe-1812655ce8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Which hardware accelerators are supported by RHEL AI?\"\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4dd992-57fe-47c3-8f28-59462c330a90",
   "metadata": {},
   "source": [
    "## Process answers from ground truth QnA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6be42-65ab-4d89-9d0c-6caf6e59cebc",
   "metadata": {},
   "source": [
    "### Load test config and qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b505d-6c04-4b29-b046-76648c6b588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)\n",
    "# llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e1bdb-8841-4e83-a620-2b239f35281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"../data_preparation/document_collection\"\n",
    "\n",
    "\n",
    "qna_list = []\n",
    "\n",
    "for file_path in Path(directory).rglob('qna.yaml'):\n",
    "    print(file_path)\n",
    "    if not file_path.name == 'qna.yaml':\n",
    "        continue\n",
    "    with open(file_path) as file:\n",
    "        qna = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        for seed_example in qna[\"seed_examples\"]:\n",
    "            for questions_and_answers in seed_example[\"questions_and_answers\"]:\n",
    "                qna_list.append(\n",
    "                    {\n",
    "                        \"question\": questions_and_answers[\"question\"].strip(),\n",
    "                        \"ground_truth\": questions_and_answers[\"answer\"].strip()                     \n",
    "                    }\n",
    "                )\n",
    "                \n",
    "# print(qna_list)\n",
    "\n",
    "qna_df = pd.DataFrame(qna_list)\n",
    "# df.to_csv('qna.csv', index=False)\n",
    "qna_df.to_json('qna.jsonl', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fcaaf-9114-4cb1-8112-53af98e11802",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "def replace_special_char(original_str):\n",
    "    return re.sub(r\"[^\\w]\", \"_\", original_str)\n",
    "\n",
    "\n",
    "def qna_request(template_str, question):\n",
    "    # print(\"QnA\")\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            qna_template = PromptTemplate.from_template(template_str)\n",
    "            conversation = LLMChain(llm=llm,\n",
    "                                    prompt=qna_template,\n",
    "                                    verbose=False\n",
    "                                   )\n",
    "            question = row[\"question\"]\n",
    "            answer = conversation.predict(question=question)\n",
    "            return answer.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt + 1 < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "def rag_request(template_str, question):\n",
    "    # print(\"RAG\")\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            rag_template = PromptTemplate.from_template(template_str)\n",
    "            rag_chain = RetrievalQA.from_chain_type(\n",
    "                llm,\n",
    "                retriever=db.as_retriever(\n",
    "                    search_type=\"similarity\",\n",
    "                    search_kwargs={\"k\": 4}\n",
    "                    ),\n",
    "                chain_type_kwargs={\"prompt\": rag_template},\n",
    "                return_source_documents=True\n",
    "                )\n",
    "            question = row[\"question\"]\n",
    "            response = rag_chain.invoke({\"query\": question})\n",
    "            return response[\"result\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\"           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba1106-731a-4477-877e-2c0769fb6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)\n",
    "    \n",
    "qna_df = pd.read_json(\"qna.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    answers = qna_df.copy()\n",
    "    answers[\"answer\"] = \"\"\n",
    "    answers[\"rag_answer\"] = \"\"\n",
    "    llm = VLLMOpenAI(\n",
    "        openai_api_key=re.sub(r\"\\s+\", \"\", testing_config[\"api_key\"]),\n",
    "        openai_api_base=testing_config[\"endpoint_url\"], #https://model...com/v1\n",
    "        model_name=testing_config[\"model_name\"],\n",
    "        temperature=0.00,\n",
    "        max_tokens=2048,\n",
    "        streaming=False\n",
    "    )\n",
    "    for index, row in answers.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        print(index, question)\n",
    "        if testing_config[\"qna_template\"]:\n",
    "            answer = qna_request(testing_config[\"qna_template\"], question)\n",
    "            # print(answer)\n",
    "            answers.at[index, \"answer\"] = answer\n",
    "        if testing_config[\"rag_template\"]:\n",
    "            answer = rag_request(testing_config[\"rag_template\"], question)\n",
    "            # print(answer)\n",
    "            answers.at[index, \"rag_answer\"] = answer\n",
    "    base_filename = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    answers.to_json(f\"{base_filename}_answers.jsonl\", orient=\"records\", lines=True)\n",
    "    # answers.to_csv(f\"{base_filename}_answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf0670-0bea-4807-b760-bf442824a17a",
   "metadata": {},
   "source": [
    "## Grade responses using Judge Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f7d9f-6f5d-439f-a5cc-42aa550d8e8b",
   "metadata": {},
   "source": [
    "### Load \\*_answers.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa5c97-b204-474d-a395-f3acbfb50483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fde059-0fb1-4075-8e23-8609a33cad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "SCORING_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"You are an evaluation system tasked with assessing the answer quality of a AI generated response in relation to the posed question and reference answer. Assess if the response is correct, accurate, and factual based on the reference answer. Evaluate the answer_quality as:\n",
    "    - Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "    - Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "    - Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "    - Score 4: The response is mostly correct, accurate, and factual.\n",
    "    - Score 5: The response is completely correct, accurate, and factual.\n",
    "    Here is the question: \\n ------- \\n {question} \\n -------\n",
    "    Here is model answer: \\n ------- \\n {answer} \\n -------\n",
    "    Here is the reference answer(may be very short and lack details or indirect, long and extractive):  \\n ------- \\n {reference_answer} \\n ------- \\n\n",
    "    Assess the quality of model answer with respect to the Reference Answer, but do not penalize the model answer for adding details or give a direct answer to user question. Provide the quality level as a JSON object with two keys: 'reasoning' and 'answer_quality'.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"answer\", \"reference_answer\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877508f-fa77-4d53-8de3-d0ba4f837281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "\n",
    "\n",
    "def score_request(question, answer, reference_answer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SCORING_PROMPT.format(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "                reference_answer=reference_answer\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = judge_client.chat.completions.create(\n",
    "        model=judge_model_name,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    response_content = completion.choices[0].message.content\n",
    "    result = json.loads(response_content)\n",
    "    score = result[\"answer_quality\"]\n",
    "    reasoning = result[\"reasoning\"]\n",
    "    return score, reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dcdb86-5272-45d6-92f4-71ea7e412599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    base_filename = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    answers_filename = f\"{base_filename}_answers.jsonl\"\n",
    "    scores = pd.read_json(answers_filename, orient=\"records\", lines=True)\n",
    "    position = scores.columns.get_loc(\"answer\")\n",
    "    scores.insert(position + 1, \"answer_score\", \"\")\n",
    "    scores.insert(position + 2, \"answer_score_reasoning\", \"\")\n",
    "    position = scores.columns.get_loc(\"rag_answer\")\n",
    "    scores.insert(position + 1, \"rag_answer_score\", \"\")\n",
    "    scores.insert(position + 2, \"rag_answer_score_reasoning\", \"\")\n",
    "\n",
    "    for index, row in scores.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "        reference_answer = row[\"ground_truth\"]\n",
    "        print(index, question)\n",
    "        if answer:\n",
    "            score, reasoning = score_request(question, answer, reference_answer)\n",
    "            scores.at[index, \"answer_score\"] = score\n",
    "            scores.at[index, \"answer_score_reasoning\"] = reasoning\n",
    "            print(answer[:40], score, reasoning[:40])\n",
    "        rag_answer = row[\"rag_answer\"]\n",
    "        if rag_answer:\n",
    "            score, reasoning = score_request(question, rag_answer, reference_answer)\n",
    "            scores.at[index, \"rag_answer_score\"] = score\n",
    "            scores.at[index, \"rag_answer_score_reasoning\"] = reasoning\n",
    "            print(rag_answer[:40], score, reasoning[:40])\n",
    "\n",
    "    judge_filename = replace_special_char(judge_model_name)\n",
    "    scores_filename = f\"{base_filename}_{judge_filename}_scores.jsonl\"\n",
    "    scores.to_json(scores_filename, orient=\"records\", lines=True)\n",
    "    scores.to_csv(f\"{base_filename}_{judge_filename}_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9ad25-b957-4fab-aea9-b9b828e030d2",
   "metadata": {},
   "source": [
    "## Create resulting score report CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cedae-4bbf-419d-87cb-d5aedf404694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)\n",
    "\n",
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "judge_filename = replace_special_char(judge_model_name)\n",
    "\n",
    "summary_output_df = pd.DataFrame()\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    base_filename = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    scores_filename = f\"{base_filename}_{judge_filename}_scores.jsonl\"\n",
    "    scores = pd.read_json(scores_filename, orient=\"records\", lines=True)\n",
    "    if testing_config[\"qna_template\"]:\n",
    "        summary_output_df[f\"{base_filename}_answer_score\"] = scores[\"answer_score\"]\n",
    "    if testing_config[\"rag_template\"]:\n",
    "        summary_output_df[f\"{base_filename}_rag_answer_score\"] = scores[\"rag_answer_score\"]\n",
    "\n",
    "\n",
    "summary_output_df.to_json(f\"summary_{judge_filename}_scores.jsonl\", orient=\"records\", lines=True)\n",
    "summary_output_df.to_csv(f\"summary_{judge_filename}_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f4e22-4218-4601-a5a9-756f9fffdc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4fa67-0db6-413d-8efd-ec74e6644751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
