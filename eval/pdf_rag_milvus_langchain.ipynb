{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb24d2-f2f7-455f-8854-6d1ae1f233b8",
   "metadata": {},
   "source": [
    "# Evaluating the fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a3532-1ed4-4671-bea4-5e35f64da2bf",
   "metadata": {},
   "source": [
    "### Model inference parameters\n",
    "\n",
    "The parameters to the fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b4bcd-ed99-4d70-aa8a-b4586a9a4966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Iterator\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fa004651738c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca2a6-523d-4a83-a102-270f5747bb00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS=2048\n",
    "TEMPERATURE=0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057b500141cb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep -E \"docling|langchain|dotenv|torch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2c060-ea3e-4bbe-8f3a-3c2e5605c2e4",
   "metadata": {},
   "source": [
    "### Milvus connection info\n",
    "\n",
    "Defaults to local db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655969f-53fb-4917-8a7e-1ee480402524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_URI = os.getenv(\"MILVUS_URI\", \"./milvus_local.db\")\n",
    "MILVUS_USERNAME = os.getenv(\"MILVUS_USERNAME\", \"\")\n",
    "MILVUS_PASSWORD = os.getenv(\"MILVUS_PASSWORD\", \"\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\", \"my_org_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420575b-4d00-458b-aa0e-7030008ccd53",
   "metadata": {},
   "source": [
    "## Sanity check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e7d20fe421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm(testing_config):\n",
    "    return VLLMOpenAI(\n",
    "        openai_api_key=re.sub(r\"\\s+\", \"\", testing_config[\"api_key\"]),\n",
    "        openai_api_base=testing_config[\"endpoint_url\"], #https://model...com/v1\n",
    "        model_name=testing_config[\"model_name\"],\n",
    "        temperature=0.00,\n",
    "        max_tokens=2048,\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "def qna_request(llm, template_str, question):\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            prompt = PromptTemplate.from_template(template_str)\n",
    "            chain = prompt | llm | StrOutputParser()\n",
    "            answer = chain.invoke({\"question\": question})        \n",
    "            print(answer)\n",
    "            return answer.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt + 1 < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\"           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69188d06-d31e-4239-8be8-fdda029bfc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)\n",
    "    \n",
    "llm = create_llm(llm_config[\"testing_config\"][0])\n",
    "template_str = llm_config[\"testing_config\"][0][\"qna_template\"]\n",
    "\n",
    "question = \"Which hardware accelerators are supported by RHEL AI?\"\n",
    "qna_request(llm, template_str, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961a7ae-d9d9-4cbc-9926-e1acaa6bc0f0",
   "metadata": {},
   "source": [
    "## Creating an Milvus DB with documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f598eb-8665-4240-91c0-3cc178aad88c",
   "metadata": {},
   "source": [
    "## Initial index creation and document ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff5f7-c509-48db-90b5-e15815b8b530",
   "metadata": {},
   "source": [
    "#### Load pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db9a89a16c5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoclingPDFLoader(BaseLoader):\n",
    "\n",
    "    def __init__(self, file_path: str | list[str]) -> None:\n",
    "        self._file_paths = file_path if isinstance(file_path, list) else [file_path]\n",
    "        self._converter = DocumentConverter()\n",
    "\n",
    "    def lazy_load(self) -> Iterator[LCDocument]:\n",
    "        for source in self._file_paths:\n",
    "            dl_doc = self._converter.convert(source).document\n",
    "            text = dl_doc.export_to_markdown()\n",
    "            yield LCDocument(page_content=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a968a5dd3653176",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"../data_preparation/document_collection\"\n",
    "file_paths = [str(path) for path in Path(pdf_folder_path).rglob('*.pdf')]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a38809-2915-4376-a847-0cec2abbfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DoclingPDFLoader(file_path=file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fe0a-38bf-4cd4-af7d-35b41c645edd",
   "metadata": {},
   "source": [
    "#### Split documents into chunks with some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362934767fd34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-c670-4eb5-803b-b4d591fa83db",
   "metadata": {},
   "source": [
    "#### Create the index and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb6d94-8615-4d55-b5b1-102f9ce56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": device}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": MILVUS_URI,\n",
    "        \"user\": MILVUS_USERNAME, \n",
    "        \"password\": MILVUS_PASSWORD\n",
    "    },\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    auto_id=True,\n",
    "    drop_old=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf425b-dffd-4f42-9537-49d41383182d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded = db.add_documents(all_splits)\n",
    "print(f\"{len(loaded)} documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3b458-4979-46df-8493-7496764a2568",
   "metadata": {},
   "source": [
    "#### Test vector DB search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c6e6d-c42c-4de4-87cf-8edfd0e63da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Which hardware accelerators are supported by RHEL AI?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90feeb37-7888-4c5f-a5cb-5f82637cec16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d47df1-ec9b-4571-8864-0c2ff1051dbf",
   "metadata": {},
   "source": [
    "#### Test out RAG request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b702a5f57bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def rag_request(llm, template_str, question):\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            prompt = PromptTemplate.from_template(template_str)\n",
    "            rag_chain = (\n",
    "                    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                    | prompt\n",
    "                    | llm\n",
    "                    | StrOutputParser()\n",
    "            )\n",
    "            response = rag_chain.invoke(question)\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7855c9fe2ced63",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = create_llm(llm_config[\"testing_config\"][0])\n",
    "template_str = llm_config[\"testing_config\"][0][\"rag_template\"]\n",
    "\n",
    "question = \"Which hardware accelerators are supported by RHEL AI?\"\n",
    "result = rag_request(llm, template_str, question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e1bdb-8841-4e83-a620-2b239f35281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data_preparation/document_collection\"\n",
    "\n",
    "\n",
    "qna_list = []\n",
    "\n",
    "for file_path in Path(directory).rglob('qna.yaml'):\n",
    "    print(file_path)\n",
    "    if not file_path.name == 'qna.yaml':\n",
    "        continue\n",
    "    with open(file_path) as file:\n",
    "        qna = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        for seed_example in qna[\"seed_examples\"]:\n",
    "            for questions_and_answers in seed_example[\"questions_and_answers\"]:\n",
    "                qna_list.append(\n",
    "                    {\n",
    "                        \"question\": questions_and_answers[\"question\"].strip(),\n",
    "                        \"ground_truth\": questions_and_answers[\"answer\"].strip()                     \n",
    "                    }\n",
    "                )\n",
    "                \n",
    "# print(qna_list)\n",
    "\n",
    "qna_df = pd.DataFrame(qna_list)\n",
    "# df.to_csv('qna.csv', index=False)\n",
    "qna_df.to_json('qna.jsonl', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba1106-731a-4477-877e-2c0769fb6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def replace_special_char(original_str):\n",
    "    return re.sub(r\"[^\\w]\", \"_\", original_str)\n",
    "\n",
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)\n",
    "    \n",
    "qna_df = pd.read_json(\"qna.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    answers = qna_df.copy()\n",
    "    answers[\"answer\"] = \"\"\n",
    "    answers[\"rag_answer\"] = \"\"\n",
    "    llm = create_llm(testing_config)\n",
    "    for index, row in answers.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        print(index, question)\n",
    "        if testing_config.get(\"qna_template\"):\n",
    "            answer = qna_request(llm, testing_config.get(\"qna_template\"), question)\n",
    "            print(\"QnA Answer: \" + answer[:40])\n",
    "            answers.at[index, \"answer\"] = answer\n",
    "        if testing_config.get(\"rag_template\"):\n",
    "            answer = rag_request(llm, testing_config.get(\"rag_template\"), question)\n",
    "            print(\"RAG Answer: \" + answer[:40])\n",
    "            answers.at[index, \"rag_answer\"] = answer\n",
    "    base_filename = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    answers.to_json(f\"{base_filename}_answers.jsonl\", orient=\"records\", lines=True)\n",
    "    # answers.to_csv(f\"{base_filename}_answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf0670-0bea-4807-b760-bf442824a17a",
   "metadata": {},
   "source": [
    "## Grade responses using Judge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa5c97-b204-474d-a395-f3acbfb50483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fde059-0fb1-4075-8e23-8609a33cad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "SCORING_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"You are an evaluation system tasked with assessing the answer quality of a AI generated response in relation to the posed question and reference answer. Assess if the response is correct, accurate, and factual based on the reference answer. Evaluate the answer_quality as:\n",
    "    - Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "    - Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "    - Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "    - Score 4: The response is mostly correct, accurate, and factual.\n",
    "    - Score 5: The response is completely correct, accurate, and factual.\n",
    "    Here is the question: \\n ------- \\n {question} \\n -------\n",
    "    Here is model answer: \\n ------- \\n {answer} \\n -------\n",
    "    Here is the reference answer(may be very short and lack details or indirect, long and extractive):  \\n ------- \\n {reference_answer} \\n ------- \\n\n",
    "    Assess the quality of model answer with respect to the Reference Answer, but do not penalize the model answer for adding details or give a direct answer to user question. Provide the quality level as a JSON object with two keys: 'reasoning' and 'answer_quality'.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"answer\", \"reference_answer\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877508f-fa77-4d53-8de3-d0ba4f837281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "\n",
    "\n",
    "def score_request(question, answer, reference_answer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SCORING_PROMPT.format(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "                reference_answer=reference_answer\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = judge_client.chat.completions.create(\n",
    "        model=judge_model_name,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    response_content = completion.choices[0].message.content\n",
    "    result = json.loads(response_content)\n",
    "    score = result[\"answer_quality\"]\n",
    "    reasoning = result[\"reasoning\"]\n",
    "    return score, reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dcdb86-5272-45d6-92f4-71ea7e412599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    base_filename = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    answers_filename = f\"{base_filename}_answers.jsonl\"\n",
    "    scores = pd.read_json(answers_filename, orient=\"records\", lines=True)\n",
    "    position = scores.columns.get_loc(\"answer\")\n",
    "    scores.insert(position + 1, \"answer_score\", \"\")\n",
    "    scores.insert(position + 2, \"answer_score_reasoning\", \"\")\n",
    "    position = scores.columns.get_loc(\"rag_answer\")\n",
    "    scores.insert(position + 1, \"rag_answer_score\", \"\")\n",
    "    scores.insert(position + 2, \"rag_answer_score_reasoning\", \"\")\n",
    "\n",
    "    for index, row in scores.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "        reference_answer = row[\"ground_truth\"]\n",
    "        print(index, question)\n",
    "        if answer:\n",
    "            score, reasoning = score_request(question, answer, reference_answer)\n",
    "            scores.at[index, \"answer_score\"] = score\n",
    "            scores.at[index, \"answer_score_reasoning\"] = reasoning\n",
    "            print(answer[:40], score, reasoning[:40])\n",
    "        rag_answer = row[\"rag_answer\"]\n",
    "        if rag_answer:\n",
    "            score, reasoning = score_request(question, rag_answer, reference_answer)\n",
    "            scores.at[index, \"rag_answer_score\"] = score\n",
    "            scores.at[index, \"rag_answer_score_reasoning\"] = reasoning\n",
    "            print(rag_answer[:40], score, reasoning[:40])\n",
    "\n",
    "    judge_filename = replace_special_char(judge_model_name)\n",
    "    scores_filename = f\"{base_filename}_{judge_filename}_scores.jsonl\"\n",
    "    scores.to_json(scores_filename, orient=\"records\", lines=True)\n",
    "    scores.to_csv(f\"{base_filename}_{judge_filename}_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9ad25-b957-4fab-aea9-b9b828e030d2",
   "metadata": {},
   "source": [
    "## Create resulting score report CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cedae-4bbf-419d-87cb-d5aedf404694",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm_config.yaml\", \"r\") as f:\n",
    "    llm_config = yaml.safe_load(f)\n",
    "\n",
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "judge_filename = replace_special_char(judge_model_name)\n",
    "\n",
    "summary_output_df = pd.DataFrame()\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    base_filename = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    scores_filename = f\"{base_filename}_{judge_filename}_scores.jsonl\"\n",
    "    scores = pd.read_json(scores_filename, orient=\"records\", lines=True)\n",
    "    if testing_config.get(\"qna_template\"):\n",
    "        summary_output_df[f\"{base_filename}_answer_score\"] = scores[\"answer_score\"]\n",
    "    if testing_config.get(\"rag_template\"):\n",
    "        summary_output_df[f\"{base_filename}_rag_answer_score\"] = scores[\"rag_answer_score\"]\n",
    "\n",
    "\n",
    "summary_output_df.to_json(f\"summary_{judge_filename}_scores.jsonl\", orient=\"records\", lines=True)\n",
    "summary_output_df.to_csv(f\"summary_{judge_filename}_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
