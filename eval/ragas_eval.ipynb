{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb24d2-f2f7-455f-8854-6d1ae1f233b8",
   "metadata": {},
   "source": [
    "# Evaluating the fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a3532-1ed4-4671-bea4-5e35f64da2bf",
   "metadata": {},
   "source": [
    "### Model inference parameters\n",
    "\n",
    "The parameters to the fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b4bcd-ed99-4d70-aa8a-b4586a9a4966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from typing import Iterator\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "def replace_special_char(original_str):\n",
    "    return re.sub(r\"[^\\w]\", \"_\", original_str)\n",
    "\n",
    "def get_config():\n",
    "    with open(\"llm_config.yaml\", \"r\") as f:\n",
    "        llm_config = yaml.safe_load(f)\n",
    "    return llm_config\n",
    "\n",
    "def get_output_dir():\n",
    "    llm_config = get_config()\n",
    "\n",
    "    output_directory = \"ragas_\" + replace_special_char(llm_config.get(\"name\", \"output\"))\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    return output_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fa004651738c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca2a6-523d-4a83-a102-270f5747bb00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS=2048\n",
    "TEMPERATURE=0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2c060-ea3e-4bbe-8f3a-3c2e5605c2e4",
   "metadata": {},
   "source": [
    "### Milvus connection info\n",
    "\n",
    "Defaults to local db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655969f-53fb-4917-8a7e-1ee480402524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_URI = os.getenv(\"MILVUS_URI\", \"./milvus_ragas_eval.db\")\n",
    "MILVUS_USERNAME = os.getenv(\"MILVUS_USERNAME\", \"\")\n",
    "MILVUS_PASSWORD = os.getenv(\"MILVUS_PASSWORD\", \"\")\n",
    "MILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\", \"my_org_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420575b-4d00-458b-aa0e-7030008ccd53",
   "metadata": {},
   "source": [
    "## Sanity check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e7d20fe421c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm(testing_config):\n",
    "    if testing_config.get(\"model_type\") == \"openai\":\n",
    "        print(\"Creating OpenAI model\")\n",
    "        return ChatOpenAI(\n",
    "            openai_api_key=re.sub(r\"\\s+\", \"\", testing_config[\"api_key\"]),\n",
    "            model=testing_config[\"model_name\"],\n",
    "            streaming=False\n",
    "        )\n",
    "    print(\"Creating VLLM model\")\n",
    "    return VLLMOpenAI(\n",
    "        openai_api_key=re.sub(r\"\\s+\", \"\", testing_config[\"api_key\"]),\n",
    "        openai_api_base=testing_config[\"endpoint_url\"], #https://model...com/v1\n",
    "        model_name=testing_config[\"model_name\"],\n",
    "        temperature=0.00,\n",
    "        max_tokens=2048,\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "def qna_request(llm, template_str, question):\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            prompt = PromptTemplate.from_template(template_str)\n",
    "            chain = prompt | llm | StrOutputParser()\n",
    "            answer = chain.invoke({\"question\": question})\n",
    "            print(answer)\n",
    "            return answer.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt + 1 < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16788d30e2db2262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "llm = create_llm(llm_config[\"testing_config\"][0])\n",
    "\n",
    "question = \"When will the ITS Telecommuting program end?\"\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69188d06-d31e-4239-8be8-fdda029bfc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_str = llm_config[\"testing_config\"][0][\"qna_template\"]\n",
    "qna_request(llm, template_str, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961a7ae-d9d9-4cbc-9926-e1acaa6bc0f0",
   "metadata": {},
   "source": [
    "## Creating an Milvus DB with documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f598eb-8665-4240-91c0-3cc178aad88c",
   "metadata": {},
   "source": [
    "## Initial index creation and document ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff5f7-c509-48db-90b5-e15815b8b530",
   "metadata": {},
   "source": [
    "#### Load pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db9a89a16c5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoclingPDFLoader(BaseLoader):\n",
    "\n",
    "    def __init__(self, file_path: str | list[str]) -> None:\n",
    "        self._file_paths = file_path if isinstance(file_path, list) else [file_path]\n",
    "        self._converter = DocumentConverter()\n",
    "\n",
    "    def lazy_load(self) -> Iterator[LCDocument]:\n",
    "        for source in self._file_paths:\n",
    "            dl_doc = self._converter.convert(source).document\n",
    "            text = dl_doc.export_to_markdown()\n",
    "            yield LCDocument(page_content=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a968a5dd3653176",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"../data_preparation/document_collection\"\n",
    "file_paths = [str(path) for path in Path(pdf_folder_path).rglob('*.pdf')]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a38809-2915-4376-a847-0cec2abbfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DoclingPDFLoader(file_path=file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198fe0a-38bf-4cd4-af7d-35b41c645edd",
   "metadata": {},
   "source": [
    "#### Split documents into chunks with some overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362934767fd34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7eae2-c670-4eb5-803b-b4d591fa83db",
   "metadata": {},
   "source": [
    "#### Create the index and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb6d94-8615-4d55-b5b1-102f9ce56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": device}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": MILVUS_URI,\n",
    "        \"user\": MILVUS_USERNAME, \n",
    "        \"password\": MILVUS_PASSWORD\n",
    "    },\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    auto_id=True,\n",
    "    drop_old=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf425b-dffd-4f42-9537-49d41383182d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded = db.add_documents(all_splits)\n",
    "print(f\"{len(loaded)} documents loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3b458-4979-46df-8493-7496764a2568",
   "metadata": {},
   "source": [
    "#### Test vector DB search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c6e6d-c42c-4de4-87cf-8edfd0e63da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What percentage of existing State-related debt is projected to be retired in 15 years?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90feeb37-7888-4c5f-a5cb-5f82637cec16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d47df1-ec9b-4571-8864-0c2ff1051dbf",
   "metadata": {},
   "source": [
    "#### Test out RAG request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b85ff51c782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def rag_request(llm, template_str, question):\n",
    "    num_retries = 1\n",
    "    for attempt in range(num_retries):\n",
    "        try:\n",
    "            search_results = db.similarity_search(question)\n",
    "            contexts = [result.page_content for result in search_results]\n",
    "            context_str = \"\\n\\n\".join(contexts)\n",
    "            prompt = PromptTemplate.from_template(template_str)\n",
    "            chain = prompt | llm | StrOutputParser()\n",
    "            response = chain.invoke({\"question\": question, \"context\": context_str})\n",
    "            return response.strip(), contexts\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt < num_retries:\n",
    "                print(f\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\", \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a1a21ae218c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "llm = create_llm(llm_config[\"testing_config\"][0])\n",
    "template_str = llm_config[\"testing_config\"][0][\"rag_template\"]\n",
    "\n",
    "question = \"What percentage of existing State-related debt is projected to be retired in 15 years?\"\n",
    "answer, contexts = rag_request(llm, template_str, question)\n",
    "print(answer)\n",
    "for context in contexts: print(\"-------\\n\" + context[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe9c8d763f89d3",
   "metadata": {},
   "source": [
    "## Generate Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44deb49353ef15",
   "metadata": {},
   "source": [
    "### Use qna.yaml to create some questions and ground truth answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e1bdb-8841-4e83-a620-2b239f35281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"../data_preparation/document_collection\"\n",
    "output_directory = get_output_dir()\n",
    "\n",
    "qna_list = []\n",
    "\n",
    "for file_path in Path(directory).rglob('qna.yaml'):\n",
    "    print(file_path)\n",
    "    if not file_path.name == 'qna.yaml':\n",
    "        continue\n",
    "    with open(file_path) as file:\n",
    "        qna = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        for seed_example in qna[\"seed_examples\"]:\n",
    "            for questions_and_answers in seed_example[\"questions_and_answers\"]:\n",
    "                qna_list.append(\n",
    "                    {\n",
    "                        \"question\": questions_and_answers[\"question\"].strip(),\n",
    "                        \"ground_truth\": questions_and_answers[\"answer\"].strip()                     \n",
    "                    }\n",
    "                )\n",
    "                \n",
    "# print(qna_list)\n",
    "\n",
    "qna_df = pd.DataFrame(qna_list)\n",
    "# df.to_csv('qna.csv', index=False)\n",
    "qna_df.to_json(f\"{output_directory}/qna.jsonl\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621433ec840ce28b",
   "metadata": {},
   "source": [
    "## Get responses from each of the available models with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba1106-731a-4477-877e-2c0769fb6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "qna_df = pd.read_json(f\"{output_directory}/qna.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    answers = qna_df.copy()\n",
    "    answers[\"contexts\"] = None\n",
    "    answers[\"answer\"] = None\n",
    "    llm = create_llm(testing_config)\n",
    "    for index, row in answers.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        print(index, question)\n",
    "        if testing_config.get(\"rag_template\"):\n",
    "            answer, contexts = rag_request(llm, testing_config.get(\"rag_template\"), question)\n",
    "            print(\"RAG Answer: \" + answer[:40])\n",
    "            answers.at[index, \"answer\"] = answer\n",
    "            answers.at[index, \"contexts\"] = contexts\n",
    "    testing_config_name = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    answers.to_json(f\"{output_directory}/{testing_config_name}_answers.jsonl\", orient=\"records\", lines=True)\n",
    "    # answers.to_csv(f\"{output_directory}/{base_filename}_answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf0670-0bea-4807-b760-bf442824a17a",
   "metadata": {},
   "source": [
    "## Grade responses using Judge Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa7aee2c8c6a24",
   "metadata": {},
   "source": [
    "## Evaluation with Ragas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21479ef02dae7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "llm_config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "\n",
    "JUDGE_API_KEY = llm_config.get(\"judge\").get(\"api_key\")\n",
    "JUDGE_MODEL_NAME = llm_config.get(\"judge\").get(\"model_name\")\n",
    "\n",
    "# TODO: set the api key in the llm and embeddings directly\n",
    "os.environ[\"OPENAI_API_KEY\"] = JUDGE_API_KEY\n",
    "\n",
    "judge_llm = ChatOpenAI(model=JUDGE_MODEL_NAME)\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(judge_llm)\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e177c85c9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    FactualCorrectness,\n",
    "    SemanticSimilarity,\n",
    "    RougeScore\n",
    ")\n",
    "from ragas import evaluate\n",
    "\n",
    "metrics = [\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings),\n",
    "    RougeScore(),\n",
    "]\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    testing_config_name = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    answers_filename = f\"{output_directory}/{testing_config_name}_answers.jsonl\"\n",
    "    answers_df = pd.read_json(answers_filename, orient=\"records\", lines=True)\n",
    "    answers_dataset = Dataset.from_pandas(answers_df)\n",
    "    scores_dataset = evaluate(dataset=answers_dataset, metrics=metrics)\n",
    "    scores = scores_dataset.to_pandas()\n",
    "    scores_filename = f\"{output_directory}/{testing_config_name}_scores\"\n",
    "    scores.to_json(f\"{scores_filename}.jsonl\", orient=\"records\", lines=True)\n",
    "    scores.to_csv(f\"{scores_filename}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9ad25-b957-4fab-aea9-b9b828e030d2",
   "metadata": {},
   "source": [
    "## Create resulting score report CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cedae-4bbf-419d-87cb-d5aedf404694",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "\n",
    "judge_client = OpenAI(api_key=llm_config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = llm_config[\"judge\"][\"model_name\"]\n",
    "judge_name = replace_special_char(judge_model_name)\n",
    "\n",
    "summary_output_df = pd.DataFrame()\n",
    "\n",
    "for testing_config in llm_config[\"testing_config\"]:\n",
    "    if not testing_config.get(\"rag_template\"):\n",
    "        continue\n",
    "    testing_config_name = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "    scores_filename = f\"{output_directory}/{testing_config_name}_scores.jsonl\"\n",
    "    scores = pd.read_json(scores_filename, orient=\"records\", lines=True)\n",
    "    summary_output_df[f\"{testing_config_name}_factual_correctness\"] = scores.get(\"factual_correctness\")\n",
    "    # summary_output_df[f\"{testing_config_name}_semantic_similarity\"] = scores.get(\"semantic_similarity\")\n",
    "    # summary_output_df[f\"{testing_config_name}_rouge_score\"] = scores.get(\"rouge_score\")\n",
    "\n",
    "average_row = summary_output_df.mean(axis=0, numeric_only=True)\n",
    "print(average_row)\n",
    "summary_output_df.loc[len(summary_output_df)] = average_row\n",
    "question_indices = [f\"Q{i + 1}\" for i in range(len(summary_output_df) - 1)]\n",
    "question_indices.append(\"Average\")\n",
    "summary_output_df.insert(0, 'question index', question_indices)\n",
    "\n",
    "summary_filepath = f\"{output_directory}/summary_{judge_name}_scores\"\n",
    "summary_output_df.to_json(f\"{summary_filepath}.jsonl\", orient=\"records\", lines=True)\n",
    "summary_output_df.to_csv(f\"{summary_filepath}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bb2e5bde823e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f\"{output_directory}/{judge_name}_scores.xlsx\") as writer:\n",
    "    summary_output_df = pd.read_csv(f\"{summary_filepath}.csv\")\n",
    "    summary_output_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "    for testing_config in llm_config[\"testing_config\"]:\n",
    "        testing_config_name = replace_special_char(testing_config[\"name\" or \"model_name\"])\n",
    "        scores_filename = f\"{output_directory}/{testing_config_name}_scores.jsonl\"\n",
    "        scores = pd.read_json(scores_filename, orient=\"records\", lines=True)\n",
    "        scores.to_excel(writer, sheet_name=f\"{testing_config_name}_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd3392-ad9d-4a6f-adf0-a21f4aa097c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
