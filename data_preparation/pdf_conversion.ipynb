{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:30.354758Z",
     "start_time": "2024-11-22T20:47:29.436543Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:32.940118Z",
     "start_time": "2024-11-22T20:47:30.358615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Third Party\n",
    "from docling.datamodel.base_models import ConversionStatus\n",
    "from docling.datamodel.document import ConvertedDocument, DocumentConversionInput\n",
    "from docling.document_converter import DocumentConverter\n",
    "from utils.logger_config import setup_logger\n",
    "import click\n",
    "import pandas as pd\n",
    "\n",
    "# Local\n",
    "from utils.docprocessor import DocProcessor\n",
    "\n",
    "def export_documents(\n",
    "        converted_docs: Iterable[ConvertedDocument],\n",
    "        output_dir: Path,\n",
    "):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "\n",
    "    for doc in converted_docs:\n",
    "        if doc.status == ConversionStatus.SUCCESS:\n",
    "            success_count += 1\n",
    "            doc_filename = doc.input.file.stem\n",
    "\n",
    "            # Export Deep Search document JSON format:\n",
    "            with (output_dir / f\"{doc_filename}.json\").open(\"w\") as fp:\n",
    "                fp.write(json.dumps(doc.render_as_dict()))\n",
    "\n",
    "            # Export Markdown format:\n",
    "            with (output_dir / f\"{doc_filename}.md\").open(\"w\") as fp:\n",
    "                fp.write(doc.render_as_markdown())\n",
    "        else:\n",
    "            print(f\"Document {doc.input.file} failed to convert.\")\n",
    "            failure_count += 1\n",
    "\n",
    "    print(\n",
    "        f\"Processed {success_count + failure_count} docs, of which {failure_count} failed\"\n",
    "    )\n",
    "\n",
    "    return doc_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:33.019187Z",
     "start_time": "2024-11-22T20:47:33.016940Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Access the variables\n",
    "input_dir = Path(os.getenv('INPUT_DIR', 'document_collection'))\n",
    "output_dir = Path(os.getenv('OUTPUT_DIR', 'output'))\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "This demo demonstrates the process of converting raw PDF files into InstructLab Synthetic Knowledge Infusion Data using the RBC POC as an example. Follow these steps to get started with your own data.\n",
    "\n",
    "#### Steps to Get Started:\n",
    "\n",
    "1. **Organize Your Documents:**\n",
    "   - Create a new directory under the `document_collection` directory for your specific project. For example, if your project is named \"my_org,\" your directory structure should look like this:\n",
    "     ```\n",
    "     |-- document_collection\n",
    "     |   `-- my_subject\n",
    "     |       |-- my_subject_data.pdf\n",
    "     |       `-- qna.yaml\n",
    "     ```\n",
    "   - Place all your PDF files and ICL files (like `qna.yaml`) into this directory.\n",
    "\n",
    "2. **Format Your ICLs:**\n",
    "   - Ensure your ICL files contain sufficient context and question-answer pairs. We recommend including at least 5 distinct contexts, each with a minimum of 3 sets of questions and answers. More entries will improve the robustness of your data.\n",
    "    - The ICL file should be in the following format (refer to the `document_collection/my_subject/qna.yaml` file for an example):\n",
    "\n",
    "    ```yaml\n",
    "    domain: \n",
    "    document_outline: A one to two line description of the document\n",
    "    seed_examples:\n",
    "      - context: <context 1 goes here>\n",
    "        question_and_answers:\n",
    "          - question: <question 1 goes here>\n",
    "            answer: <answer 1 goes here>\n",
    "          - question: <question 2 goes here>\n",
    "            answer: <answer 2 goes here>\n",
    "          - question: <question 3 goes here>\n",
    "            answer: <answer 3 goes here>\n",
    "    ... \n",
    "\n",
    "\n",
    "   - **Note:** Replace placeholders with actual content relevant to your documents. Ensure the contexts are clear and questions are well-formulated to extract meaningful answers.\n",
    "\n",
    "3. **Update the Data Directory Path:**\n",
    "   - In the script or code where the data directory is specified, update the `input_dir` variable to reflect the path to your new directory. For example:\n",
    "     ```python\n",
    "     data_dir = \"document_collection/my_subject\"\n",
    "     ```\n",
    "4. **Update the Output Directory Path:**\n",
    "   - In the script or code where the data directory is specified, update the `output_dir` variable to reflect the path to your directory. For example:\n",
    "     ```python\n",
    "     data_dir = \"output/my_subject\"\n",
    "     ```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Documents to Seed Dataset\n",
    "\n",
    "To convert PDF documents into a usable seed dataset, we employ [Docling](https://github.com/DS4SD/docling), a tool designed for extracting and processing text from PDF files. The text extraction process involves parsing the PDF documents and saving the extracted text into a structured JSON file. The extracted text in JSON format can be used to generate InstructLab Synthetic Knowledge Infusion Data.\n",
    "\n",
    "\n",
    "#### Step 1: \n",
    "\n",
    "Run the following command to extract text from the PDF documents and save it in JSON format:\n",
    "\n",
    "⚠️ **Note:** This process takes about 5 minutes to run for this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:33.025256Z",
     "start_time": "2024-11-22T20:47:33.022978Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_directory(input_dir, output_dir):\n",
    "    file_paths = list(input_dir.rglob(\"*.pdf\"))\n",
    "    artifacts_path = DocumentConverter.download_models_hf()\n",
    "    doc_converter = DocumentConverter(artifacts_path=artifacts_path)\n",
    "    inputs = DocumentConversionInput.from_paths(file_paths)\n",
    "\n",
    "    start_time = time.time()\n",
    "    converted_docs = doc_converter.convert(inputs)\n",
    "    doc_filename = export_documents(converted_docs, output_dir)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Parsing documents took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    dp = DocProcessor(output_dir, user_config_path=f'{input_dir}/qna.yaml')\n",
    "    seed_data = dp.get_processed_dataset()\n",
    "\n",
    "    seed_data.to_json(f'{output_dir}/seed_data.jsonl', orient='records', lines=True)\n",
    "\n",
    "    md_output_dir = f\"{output_dir}/md\"\n",
    "    os.makedirs(md_output_dir, exist_ok=True)\n",
    "    jsonl_file_path = f\"{output_dir}/seed_data.jsonl\"\n",
    "    return jsonl_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:52.521366Z",
     "start_time": "2024-11-22T20:47:33.028900Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_leaf_directories(root_dir):\n",
    "    leaf_dirs = []\n",
    "    for dirpath, dirs, files in os.walk(root_dir):\n",
    "        if not dirs:\n",
    "            print(dirpath, type(dirpath))\n",
    "            leaf_dirs.append(dirpath)\n",
    "    return leaf_dirs\n",
    "\n",
    "dirs = get_leaf_directories(input_dir)\n",
    "for dir in dirs:\n",
    "    print(f\"Processing directory: {dir}\")\n",
    "    process_directory(Path(dir), Path(re.sub(str(input_dir), str(output_dir), dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_leaf_directories(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSONL to markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:52.531863Z",
     "start_time": "2024-11-22T20:47:52.526014Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_jsonl_file(jsonl_file_path):\n",
    "    output_dir = os.path.dirname(jsonl_file_path)\n",
    "    md_output_dir = f\"{output_dir}/md\"\n",
    "    os.makedirs(md_output_dir, exist_ok=True)\n",
    "\n",
    "    with open(jsonl_file_path, 'r') as f:\n",
    "        saved_hashes = set()\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            document_text = entry.get('document', '')\n",
    "            h = hash(document_text)\n",
    "            if h not in saved_hashes:\n",
    "                saved_hashes.add(h)\n",
    "                i += 1\n",
    "                file_path = os.path.join(md_output_dir, f\"document_{i}.md\")\n",
    "                with open(file_path, 'w') as f:\n",
    "                    f.write(document_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:47:52.542056Z",
     "start_time": "2024-11-22T20:47:52.536238Z"
    }
   },
   "outputs": [],
   "source": [
    "jsonl_file_paths = list(output_dir.rglob(\"seed_data.jsonl\"))\n",
    "for jsonl_file_path in jsonl_file_paths:\n",
    "    process_jsonl_file(jsonl_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy qna.yaml files into a taxonomy directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def copy_qna_files(input_dir, output_dir, overwrite=False):\n",
    "    qna_file_paths = list(input_dir.rglob(\"qna.yaml\"))\n",
    "    for src_file in qna_file_paths:\n",
    "        if src_file.is_file():\n",
    "            rel_path = src_file.relative_to(input_dir)\n",
    "            if str(rel_path).find(\"knowledge\") != -1:\n",
    "                dest_file = output_dir / \"taxonomy\" / rel_path\n",
    "            else:\n",
    "                dest_file = output_dir / \"taxonomy/knowledge\" / rel_path\n",
    "\n",
    "            if overwrite or not dest_file.exists():\n",
    "                dest_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(src_file, dest_file)\n",
    "                print(f\"Copied: {rel_path}\")\n",
    "            else:\n",
    "                print(f\"Skipped (already exists): {rel_path}\")\n",
    "\n",
    "\n",
    "copy_qna_files(input_dir, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
